---
title: "3D Scene-Aware Vision-Language Action Modeling for Robot Manipulation"
excerpt: "<br/><img src='/images/scene_aware_openvla.png'>"
collection: projects
---

Enhanced OpenVLA by integrating object detection, depth features, and chain-of-thought reasoning to improve its spatial and semantic understanding. Using Molmo VLM and SAM, I implemented object-specific attention masking, ensuring the model focuses on task-relevant objects. We further introduced depth-aware embeddings via PointNet, combining RGB and depth data to improve 3D spatial reasoning. To enhance task execution, I generated detailed GPT-4-based task narrations, breaking high-level instructions into structured action steps. These improvements boosted task success rates by 8% on the LIBERO-Long benchmark, demonstrating superior performance in long-horizon robotic manipulation. 

[Report](https://drive.google.com/file/d/1eWaFBsUesDDK84zn9NHzfuNJPMY0GTkd/view?usp=sharing) [Video](https://drive.google.com/file/d/1CRphm4elaomj_aZ38m0OC72B9yE2kzYi/view?usp=sharing) [Code will be shared soon]

---
